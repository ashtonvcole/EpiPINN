{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b1fb3c",
   "metadata": {},
   "source": [
    "# SCI ML Project\n",
    "### Antonio Jimenez aoj268\n",
    "### Ashton Cole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d382b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, hidden_size, depth):\n",
    "        super().__init__()\n",
    "        # input t\n",
    "        layers = [nn.Linear(1, hidden_size), nn.Tanh()]\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        # Output layer with 5 units for (s, e, i, r, d)\n",
    "        layers.append(nn.Linear(hidden_size, 5))\n",
    "        # Add softmax to enforce all components are positive and sum to 1\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa157c37-534a-4ea1-9491-42409c71ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caputo_l1_diff(psi, alpha, dt):\n",
    "    n = len(psi)\n",
    "    # The derivative at t=0 is undefined for the L1 scheme\n",
    "    derivatives = [torch.zeros(1, device=psi.device)] \n",
    "    \n",
    "    # Pre-compute the log of the gamma function part for stability\n",
    "    log_gamma_term = torch.lgamma(2.0 - alpha)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        # Make vector of k values from 0 to i-1\n",
    "        k = torch.arange(i, dtype=torch.float32, device=psi.device)\n",
    "        \n",
    "        # Calculate weights c_k^(i) \n",
    "        weights = ((k + 1)**(1 - alpha) - k**(1 - alpha))\n",
    "        \n",
    "        # Get the differences psi(t_{i-k}) - psi(t_{i-k-1})\n",
    "        psi_diffs = psi[i - k.long()] - psi[i - k.long() - 1]\n",
    "        \n",
    "        summation = torch.sum(weights * psi_diffs.squeeze())\n",
    "        \n",
    "        # Combine everything to get the derivative at time t_i\n",
    "        deriv_at_i = (1.0 / (dt**alpha * torch.exp(log_gamma_term))) * summation\n",
    "        derivatives.append(deriv_at_i.unsqueeze(0))\n",
    "        \n",
    "    return torch.cat(derivatives).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalSEIRD(nn.Module):\n",
    "    def __init__(self, hidden_size, depth, initial_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pinn = PINN(hidden_size, depth) \n",
    "        # trainable params\n",
    "        self.raw_beta = nn.Parameter(torch.tensor([initial_params['beta']]))\n",
    "        self.raw_sigma = nn.Parameter(torch.tensor([initial_params['sigma']]))\n",
    "        self.raw_gamma = nn.Parameter(torch.tensor([initial_params['gamma']]))\n",
    "        self.raw_mu = nn.Parameter(torch.tensor([initial_params['mu']]))\n",
    "        # Init z_alpha such that the init alpha is close to 1.0\n",
    "        self.z_alpha = nn.Parameter(torch.tensor([initial_params['z_alpha']])) # sigmoid(2.94) is approx 0.95\n",
    "\n",
    "        weight_ic = 1.0\n",
    "        weight_data = 1.0\n",
    "        weight_phys = 1.0\n",
    "        \n",
    "        self.min_alpha = initial_params['min_alpha'] # Example minimum value for alpha\n",
    "        self.dt = initial_params['dt']\n",
    "\n",
    "    def beta(self):\n",
    "        return nn.softplus(self.raw_beta)\n",
    "\n",
    "    def sigma(self):\n",
    "        return nn.softplus(self.raw_sigma)\n",
    "\n",
    "    def gamma(self):\n",
    "        return nn.softplus(self.raw_gamma)\n",
    "        \n",
    "    def mu(self):\n",
    "        return nn.softplus(self.raw_mu)\n",
    "\n",
    "    def alpha(self):\n",
    "        # Restrict alpha to a specific range, (min_alpha, 1.0] \n",
    "        return self.min_alpha + (1.0 - self.min_alpha) * torch.sigmoid(self.z_alpha)\n",
    "    \n",
    "    def forward(self,t):\n",
    "        self.pinn(t)\n",
    "\n",
    "    def compute_loss(self, t_colloc, t_data, y_data, ic):\n",
    "        # IC loss\n",
    "        t_initial = t_colloc[0].unsqueeze(0) # get t_0\n",
    "        y_initial_pred = self.forward(t_initial)\n",
    "        loss_ic = nn.MSELoss(y_initial_pred - ic)\n",
    "        \n",
    "        # Data Loss\n",
    "        y_data_pred = self.forward(t_data)\n",
    "        loss_data = nn.MSELoss(y_data_pred - y_data)\n",
    "\n",
    "        # Phys Loss\n",
    "        y_all_pred = self.forward(t_colloc)\n",
    "        s,e,i,r,d = y_all_pred.unbind(1)\n",
    "        ds_dt = caputo_l1_diff(s, self.alpha, self.dt)\n",
    "        de_dt = caputo_l1_diff(e, self.alpha, self.dt)\n",
    "        di_dt = caputo_l1_diff(i, self.alpha, self.dt)\n",
    "        dr_dt = caputo_l1_diff(r, self.alpha, self.dt)\n",
    "        dd_dt = caputo_l1_diff(d, self.alpha, self.dt)\n",
    "\n",
    "        # calculate RHS of equation 4\n",
    "        num_living =  1 - d\n",
    "        f_s = -self.beta() * s * i / num_living\n",
    "        f_e = (self.beta() * s * i / num_living) - self.sigma() * e\n",
    "        f_i = (self.sigma() * e) - (self.gamma()+ self.mu()) * i\n",
    "        f_r = self.gamma() * i\n",
    "        f_d = self.mu() * i\n",
    "\n",
    "        # calc residuals (LHS - RHS = 0)\n",
    "        residual_s = ds_dt - f_s\n",
    "        residual_e = de_dt - f_e\n",
    "        residual_i = di_dt - f_i\n",
    "        residual_r = dr_dt - f_r\n",
    "        residual_d = dd_dt - f_d\n",
    "\n",
    "        all_residuals = torch.cat([residual_s, residual_e, residual_i, residual_r, residual_d], dim=1)\n",
    "        loss_phys = torch.mean(all_residuals**2)\n",
    "\n",
    "        tot_loss = weight_ic * loss_ic + weight_data * loss_data + weight_physics * loss_phys # cons loss handled b soft max, regularization loss handled by optimizer \n",
    "        return tot_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
