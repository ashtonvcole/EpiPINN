{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b1fb3c",
   "metadata": {},
   "source": [
    "# EpiPINN\n",
    "\n",
    "Physics-informed neural network learning epidemiological parameters\n",
    "\n",
    "### Antonio Jimenez AOJ268\n",
    "### Ashton Cole AVC687\n",
    "\n",
    "## Contents\n",
    "\n",
    "- Definitions\n",
    "- Experiments\n",
    "    - Experiment 1: Parameter estimation with exact data\n",
    "    - Experiment 2: Parameter estimation with noisy data\n",
    "    - Experiment 3: Parameter estimation and forecasting with noisy limited data\n",
    "\n",
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d382b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, hidden_size, depth):\n",
    "        super().__init__()\n",
    "        # input t\n",
    "        layers = [nn.Linear(1, hidden_size), nn.Tanh()]\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        # Output layer with 5 units for (s, e, i, r, d)\n",
    "        layers.append(nn.Linear(hidden_size, 5))\n",
    "        # Add softmax to enforce all components are positive and sum to 1\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa157c37-534a-4ea1-9491-42409c71ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caputo_l1_diff(psi, alpha, dt):\n",
    "    n = len(psi)\n",
    "    # The derivative at t=0 is undefined for the L1 scheme\n",
    "    derivatives = [torch.zeros(1, device=psi.device)] \n",
    "    \n",
    "    # Pre-compute the log of the gamma function part for stability\n",
    "    log_gamma_term = torch.lgamma(2.0 - alpha)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        # Make vector of k values from 0 to i-1\n",
    "        k = torch.arange(i, dtype=torch.float32, device=psi.device)\n",
    "        \n",
    "        # Calculate weights c_k^(i) \n",
    "        weights = ((k + 1)**(1 - alpha) - k**(1 - alpha))\n",
    "        \n",
    "        # Get the differences psi(t_{i-k}) - psi(t_{i-k-1})\n",
    "        psi_diffs = psi[i - k.long()] - psi[i - k.long() - 1]\n",
    "        \n",
    "        summation = torch.sum(weights * psi_diffs.squeeze())\n",
    "        \n",
    "        # Combine everything to get the derivative at time t_i\n",
    "        deriv_at_i = (1.0 / (dt**alpha * torch.exp(log_gamma_term))) * summation\n",
    "        derivatives.append(deriv_at_i.unsqueeze(0))\n",
    "        \n",
    "    return torch.cat(derivatives).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpiPINN(nn.Module):\n",
    "    def __init__(self, hidden_size, depth, initial_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pinn = PINN(hidden_size, depth) \n",
    "        # trainable params\n",
    "        self.raw_beta = nn.Parameter(torch.tensor([initial_params['beta']]))\n",
    "        self.raw_sigma = nn.Parameter(torch.tensor([initial_params['sigma']]))\n",
    "        self.raw_gamma = nn.Parameter(torch.tensor([initial_params['gamma']]))\n",
    "        self.raw_mu = nn.Parameter(torch.tensor([initial_params['mu']]))\n",
    "        # Init z_alpha such that the init alpha is close to 1.0\n",
    "        self.z_alpha = nn.Parameter(torch.tensor([initial_params['z_alpha']])) # sigmoid(2.94) is approx 0.95\n",
    "        \n",
    "        self.min_alpha = initial_params['min_alpha'] # Example minimum value for alpha\n",
    "        self.dt = initial_params['dt']\n",
    "\n",
    "    def beta(self):\n",
    "        return nn.softplus(self.raw_beta)\n",
    "\n",
    "    def sigma(self):\n",
    "        return nn.softplus(self.raw_sigma)\n",
    "\n",
    "    def gamma(self):\n",
    "        return nn.softplus(self.raw_gamma)\n",
    "        \n",
    "    def mu(self):\n",
    "        return nn.softplus(self.raw_mu)\n",
    "\n",
    "    def alpha(self):\n",
    "        # Restrict alpha to a specific range, (min_alpha, 1.0] \n",
    "        return self.min_alpha + (1.0 - self.min_alpha) * torch.sigmoid(self.z_alpha)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        self.pinn(t)\n",
    "\n",
    "    def get_loss_ic(self, ts, ic, y_pred=None):\n",
    "        \"\"\"Get initial condition loss of model.\n",
    "\n",
    "        Arguments:\n",
    "            ts (torch.tensor): Time points for the model, assuming t[0] is the initial time. Only t[0] is needed, but the tensor dimensions must be consistent.\n",
    "            ic (torch.tensor): The initial state to enforce.\n",
    "            y_pred=None (torch.tensor): Predictions at time points, if already computed. Only y[0] is needed, but the tensor dimensions must be consistent.\n",
    "\n",
    "        Returns:\n",
    "            squared l2 norm of initial condition error\n",
    "        \"\"\"\n",
    "        # IC loss\n",
    "        t_initial = ts[0].unsqueeze(0) # get t_0\n",
    "        y_initial_pred = self.forward(t_initial) if y_pred == None else y_pred[0, :].unsqueeze(0)\n",
    "        loss_ic = nn.MSELoss(y_initial_pred - ic)\n",
    "\n",
    "    def get_loss_data(self, t_data, y_data, y_data_pred=None):\n",
    "        \"\"\"Get data loss of model.\n",
    "\n",
    "        Arguments:\n",
    "            t_data (torch.tensor): Training data times.\n",
    "            y_data (torch.tensor): Corresponding state data.\n",
    "            y_data_pred=None (torch.tensor): Predictions at data points, if already computed.\n",
    "\n",
    "        Returns:\n",
    "            MSE loss (squared l2 norm) of data error\n",
    "        \"\"\"\n",
    "        # Data Loss\n",
    "        y_data_pred = self.forward(t_data) if y_data_pred == None else y_data_pred\n",
    "        loss_data = nn.MSELoss(y_data_pred - y_data)\n",
    "\n",
    "    def get_loss_phys(self, t_colloc, y_colloc_pred=None):\n",
    "        \"\"\"Get physics loss of model.\n",
    "\n",
    "        Arguments:\n",
    "            t_colloc (torch.tensor): Times at which to compute the loss.\n",
    "            y_colloc_pred=None (torch.tensor): Predictions at collocation points, if already computed.\n",
    "\n",
    "        Returns:\n",
    "            squared l2 norm of residual\n",
    "        \"\"\"\n",
    "        # Phys Loss\n",
    "        y_colloc_pred = self.forward(t_colloc)\n",
    "        s,e,i,r,d = y_colloc_pred.unbind(1)\n",
    "        ds_dt = caputo_l1_diff(s, self.alpha, self.dt)\n",
    "        de_dt = caputo_l1_diff(e, self.alpha, self.dt)\n",
    "        di_dt = caputo_l1_diff(i, self.alpha, self.dt)\n",
    "        dr_dt = caputo_l1_diff(r, self.alpha, self.dt)\n",
    "        dd_dt = caputo_l1_diff(d, self.alpha, self.dt)\n",
    "\n",
    "        # calculate RHS of equation 4\n",
    "        num_living =  1 - d\n",
    "        f_s = -self.beta() * s * i / num_living\n",
    "        f_e = (self.beta() * s * i / num_living) - self.sigma() * e\n",
    "        f_i = (self.sigma() * e) - (self.gamma()+ self.mu()) * i\n",
    "        f_r = self.gamma() * i\n",
    "        f_d = self.mu() * i\n",
    "\n",
    "        # calc residuals (LHS - RHS = 0)\n",
    "        residual_s = ds_dt - f_s\n",
    "        residual_e = de_dt - f_e\n",
    "        residual_i = di_dt - f_i\n",
    "        residual_r = dr_dt - f_r\n",
    "        residual_d = dd_dt - f_d\n",
    "\n",
    "        all_residuals = torch.cat([residual_s, residual_e, residual_i, residual_r, residual_d], dim=1)\n",
    "        loss_phys = torch.mean(all_residuals**2)\n",
    "        return loss_phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae739152-8683-4e0f-96ec-316764a781ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(model, ts, ys, t_colloc, ic, optimizer, epochs=1000, pr=0):\n",
    "    \"\"\"Stage one of EpiPINN training process.\n",
    "\n",
    "    In this stage, only the weights of the neural network are trained to minimize the data and initial condition loss.\n",
    "\n",
    "    Arguments:\n",
    "        model (EpiPINN): An instantiated fractional SEIRD model to train.\n",
    "        ts (torch.tensor): Time values for time-series data.\n",
    "        ys (torch.tensor): States (s, e, i, r, d) for time-series data.\n",
    "        optimizer: Pytorch training optimizer.\n",
    "        epochs=1000 (Int): How many epochs to perform gradient descent.\n",
    "        pr=0 (Int): Print progress every pr epochs. If 0, nothing is printed.\n",
    "\n",
    "    Returns:\n",
    "        losses, losses_data, losses_ic, losses_phys\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    losses_data = []\n",
    "    losses_ic = []\n",
    "    losses_phys = []\n",
    "\n",
    "    # Ensure epidemiological parameters are not trained\n",
    "    model.raw_beta.requires_grad = False\n",
    "    model.raw_sigma.requires_grad = False\n",
    "    model.raw_gamma.requires_grad = False\n",
    "    model.raw_mu.requires_grad = False\n",
    "    model.z_alpha.requires_grad = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = model(ts)\n",
    "        \n",
    "        # Compute losses separately, then combine\n",
    "        loss_data = get_loss_data(ts, predictions)\n",
    "        loss_ic = get_loss_ic(t_colloc, ic)\n",
    "        loss_phys = get_loss_phys(t_colloc)\n",
    "        loss = loss_data + loss_ic # Physics is not used for gradient descent\n",
    "\n",
    "        # Record losses\n",
    "        losses.append(loss + loss_phys) # Physics is recored, not mimimized\n",
    "        losses_data.append(loss_data)\n",
    "        losses_ic.append(loss_ic)\n",
    "        losses_phys.append(loss_phys)\n",
    "\n",
    "        # Adjust weights to minimize loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress if desired\n",
    "        if pr != 0 and (epoch + 1) % pr == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "        \n",
    "    return losses, losses_data, losses_ic, losses_phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d10104-a1c3-4cf5-bec2-f8e9053aa938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(model, ts, ys, t_colloc, ic, optimizer, epochs=1000, pr=0):\n",
    "    \"\"\"Stage two of EpiPINN training process.\n",
    "\n",
    "    In this stage, both the weights of the neural network and epidemiological parameters are trained to minimize the data, initial condition, and physics losses.\n",
    "\n",
    "    Arguments:\n",
    "        model (EpiPINN): An instantiated fractional SEIRD model to train.\n",
    "        ts (torch.tensor): Time values for time-series data.\n",
    "        ys (torch.tensor): States (s, e, i, r, d) for time-series data.\n",
    "        optimizer: Pytorch training optimizer.\n",
    "        epochs=1000 (Int): How many epochs to perform gradient descent.\n",
    "        pr=0 (Int): Print progress every pr epochs. If 0, nothing is printed.\n",
    "\n",
    "    Returns:\n",
    "        losses, losses_data, losses_ic, losses_phys\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    losses_data = []\n",
    "    losses_ic = []\n",
    "    losses_phys = []\n",
    "\n",
    "    # Ensure epidemiological parameters are trained\n",
    "    model.raw_beta.requires_grad = True\n",
    "    model.raw_sigma.requires_grad = True\n",
    "    model.raw_gamma.requires_grad = True\n",
    "    model.raw_mu.requires_grad = True\n",
    "    model.z_alpha.requires_grad = True\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = model(ts)\n",
    "        \n",
    "        # Compute losses separately, then combine\n",
    "        loss_data = get_loss_data(ts, predictions)\n",
    "        loss_ic = get_loss_ic(t_colloc, ic)\n",
    "        loss_phys = get_loss_phys(t_colloc)\n",
    "        loss = loss_data + loss_ic + loss_phys\n",
    "\n",
    "        # Record losses\n",
    "        losses.append(loss)\n",
    "        losses_data.append(loss_data)\n",
    "        losses_ic.append(loss_ic)\n",
    "        losses_phys.append(loss_phys)\n",
    "\n",
    "        # Adjust weights to minimize loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress if desired\n",
    "        if pr != 0 and (epoch + 1) % pr == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "        \n",
    "    return losses, losses_data, losses_ic, losses_phys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8ce53-f989-4519-a152-99ff2b0869aa",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experiment 1: Parameter estimation with exact data\n",
    "\n",
    "This experiment is a recreation of the Mpox synthetic case found in the paper. We were unable to find the synthetic time series data $(s, e, i, r, d)(t)$ from the cited source, so we instead implemented a fractional ODE solver to generate data consistent with the provided epidemiological parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28306a-d6a5-44f5-9e41-9bb05c2e8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "alpha_true = 0.95 # Derivative fraction used for data\n",
    "beta_true = 0.25 # Infection rate used for model\n",
    "sigma_true = 0.13 # Incubation rate used for model\n",
    "gamma_true = 0.052 # Recovery rate used for model\n",
    "mu_true = 0.005 # Death rate used for model\n",
    "y0 = np.array([0.99, 0.01, 0, 0, 0]) # Initial state: 1% exposed\n",
    "t0 = 0\n",
    "tf = 500 # 500 days\n",
    "num_step = 500 # Good ground truth from tests\n",
    "\n",
    "f = lambda t, y: np.array([\n",
    "    - beta_true * (y[0] * y[3]) / (1 - y[4]),\n",
    "    beta_true * (y[0] * y[3]) / (1 - y[4]) - sigma_true * y[1],\n",
    "    sigma_true * y[1] - (gamma_true + mu_true) * y[2],\n",
    "    gamma_true * y[2],\n",
    "    mu_true * y[2]\n",
    "])\n",
    "\n",
    "ts, ys = caputo_euler(f, alpha, (t0, tf), num_step)\n",
    "\n",
    "## TODO: turn data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f4b64-0d3a-48a6-acd6-5e3d146e321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "min_alpha_guess = 0.9 # Mimimum searched derivative fraction\n",
    "alpha_guess = 1 # Derivative fraction used for model\n",
    "z_alpha_guess = torch.logit(alpha_guess)\n",
    "beta_guess = 0.25 # Infection rate used for model\n",
    "sigma_guess = 0.13 # Incubation rate used for model\n",
    "gamma_guess = 0.052 # Recovery rate used for model\n",
    "mu_guess = 0.005 # Death rate used for model\n",
    "hidden_size = 64 # Number of neurons per layer\n",
    "depth = 3 # Number of layers\n",
    "initial params = {\n",
    "    \"z_alpha\": z_alpha_guess,\n",
    "    \"min_alpha\": min_alpha_guess,\n",
    "    \"beta\": beta_guess,\n",
    "    \"sigma\": sigma_guess,\n",
    "    \"gamma\": gamma_guess,\n",
    "    \"mu\": mu_guess\n",
    "}\n",
    "\n",
    "model1 = EpiPINN(hidden_size, depth, initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3a179-3c3f-449a-9aaf-574fbe24d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if already generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb566f-c56f-460c-a0f7-a8d46a1a91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "# Stage 1: weights only, without considering physics in updates\n",
    "\n",
    "# Stage 2: weights and epidemiological parameters, considering full loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a8b00-0945-46c9-a3fc-7b63fcd74553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502ba52-25fa-4ff5-bb19-349ca34b421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound plot of model results, and comparison against data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453c751-88a1-42b4-a9b9-b95e54d88574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509ea4d-f3da-4176-9f15-304ea067be9e",
   "metadata": {},
   "source": [
    "### Experiment 2: Parameter estimation with noisy data\n",
    "\n",
    "This experiment is also a recreation of the Mpox synthetic case found in the paper. Now random noise is added to the data to test the model's abitilty to recover the ideal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8d645-6ad2-415f-8c54-2c03cb14bf97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
