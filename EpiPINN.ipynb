{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b1fb3c",
   "metadata": {},
   "source": [
    "# EpiPINN\n",
    "\n",
    "Physics-informed neural network learning epidemiological parameters\n",
    "\n",
    "### Antonio Jimenez AOJ268\n",
    "### Ashton Cole AVC687\n",
    "\n",
    "## Contents\n",
    "\n",
    "- Definitions\n",
    "- Experiments\n",
    "    - Experiment 1: Parameter estimation with exact data\n",
    "    - Experiment 2: Parameter estimation with noisy data\n",
    "    - Experiment 3: Parameter estimation and forecasting with noisy limited data\n",
    "\n",
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd3e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from caputo import caputo_euler\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d382b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, hidden_size, depth):\n",
    "        super().__init__()\n",
    "        # input t\n",
    "        layers = [nn.Linear(1, hidden_size), nn.Tanh()]\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        # Output layer with 5 units for (s, e, i, r, d)\n",
    "        layers.append(nn.Linear(hidden_size, 5))\n",
    "        # Add softmax to enforce all components are positive and sum to 1\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa157c37-534a-4ea1-9491-42409c71ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caputo_l1_diff(psi, alpha, dt):\n",
    "    n = len(psi)\n",
    "    # The derivative at t=0 is undefined for the L1 scheme\n",
    "    derivatives = [torch.zeros(1, 1, device=psi.device)] \n",
    "    \n",
    "    # Pre-compute the log of the gamma function part for stability\n",
    "    log_gamma_term = torch.lgamma(2.0 - alpha)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        # Make vector of k values from 0 to i-1\n",
    "        k = torch.arange(i, dtype=torch.float32, device=psi.device)\n",
    "        \n",
    "        # Calculate weights c_k^(i) \n",
    "        weights = ((k + 1)**(1 - alpha) - k**(1 - alpha))\n",
    "        \n",
    "        # Get the differences psi(t_{i-k}) - psi(t_{i-k-1})\n",
    "        psi_diffs = psi[i - k.long()] - psi[i - k.long() - 1]\n",
    "        \n",
    "        summation = torch.sum(weights * psi_diffs.squeeze())\n",
    "        \n",
    "        # Combine everything to get the derivative at time t_i\n",
    "        deriv_at_i = (1.0 / (dt**alpha * torch.exp(log_gamma_term))) * summation\n",
    "        derivatives.append(deriv_at_i.unsqueeze(0))\n",
    "        \n",
    "    return torch.cat(derivatives).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpiPINN(nn.Module):\n",
    "    def __init__(self, hidden_size, depth, initial_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pinn = PINN(hidden_size, depth) \n",
    "        # trainable params\n",
    "        # x + torch.log(-torch.expm1(-x))\n",
    "        self.raw_beta = nn.Parameter(torch.tensor([initial_params['beta'] + np.log(-np.expm1(-initial_params['beta']))], dtype=torch.float32))\n",
    "        self.raw_sigma = nn.Parameter(torch.tensor([initial_params['sigma'] + np.log(-np.expm1(-initial_params['sigma']))], dtype=torch.float32))\n",
    "        self.raw_gamma = nn.Parameter(torch.tensor([initial_params['gamma'] + np.log(-np.expm1(-initial_params['gamma']))], dtype=torch.float32))\n",
    "        self.raw_mu = nn.Parameter(torch.tensor([initial_params['mu'] + np.log(-np.expm1(-initial_params['mu']))], dtype=torch.float32))\n",
    "        # Init z_alpha such that the init alpha is close to 1.0\n",
    "        self.z_alpha = nn.Parameter(torch.tensor([initial_params['z_alpha']], dtype=torch.float32)) # sigmoid(2.94) is approx 0.95\n",
    "        \n",
    "        self.min_alpha = initial_params['min_alpha'] # Example minimum value for alpha\n",
    "        self.dt = initial_params['dt']\n",
    "\n",
    "    def beta(self):\n",
    "        return nn.functional.softplus(self.raw_beta)\n",
    "\n",
    "    def sigma(self):\n",
    "        return nn.functional.softplus(self.raw_sigma)\n",
    "\n",
    "    def gamma(self):\n",
    "        return nn.functional.softplus(self.raw_gamma)\n",
    "        \n",
    "    def mu(self):\n",
    "        return nn.functional.softplus(self.raw_mu)\n",
    "\n",
    "    def alpha(self):\n",
    "        # Restrict alpha to a specific range, (min_alpha, 1.0] \n",
    "        return self.min_alpha + (1.0 - self.min_alpha) * torch.sigmoid(self.z_alpha)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        return self.pinn(t)\n",
    "\n",
    "    def get_loss_ic(self, ts, ic, y_pred=None):\n",
    "        \"\"\"Get initial condition loss of model.\n",
    "\n",
    "        Arguments:\n",
    "            ts (torch.tensor): Time points for the model, assuming t[0] is the initial time. Only t[0] is needed, but the tensor dimensions must be consistent.\n",
    "            ic (torch.tensor): The initial state to enforce.\n",
    "            y_pred=None (torch.tensor): Predictions at time points, if already computed. Only y[0] is needed, but the tensor dimensions must be consistent.\n",
    "\n",
    "        Returns:\n",
    "            squared l2 norm of initial condition error\n",
    "        \"\"\"\n",
    "        # IC loss\n",
    "        t_initial = ts[0].unsqueeze(0) # get t_0\n",
    "        y_initial_pred = self.forward(t_initial) if y_pred == None else y_pred[0, :].unsqueeze(0)\n",
    "        return nn.functional.mse_loss(y_initial_pred, ic)\n",
    "\n",
    "    def get_loss_data(self, t_data, y_data, y_data_pred=None):\n",
    "        \"\"\"Get data loss of model.\n",
    "\n",
    "        Arguments:\n",
    "            t_data (torch.tensor): Training data times.\n",
    "            y_data (torch.tensor): Corresponding state data.\n",
    "            y_data_pred=None (torch.tensor): Predictions at data points, if already computed.\n",
    "\n",
    "        Returns:\n",
    "            MSE loss (squared l2 norm) of data error\n",
    "        \"\"\"\n",
    "        # Data Loss\n",
    "        y_data_pred = self.forward(t_data) if y_data_pred == None else y_data_pred\n",
    "        return nn.functional.mse_loss(y_data_pred, y_data)\n",
    "\n",
    "    def get_loss_phys(self, t_colloc, y_colloc_pred=None):\n",
    "        \"\"\"Get physics loss of model.\n",
    "\n",
    "        Arguments:\n",
    "            t_colloc (torch.tensor): Times at which to compute the loss.\n",
    "            y_colloc_pred=None (torch.tensor): Predictions at collocation points, if already computed.\n",
    "\n",
    "        Returns:\n",
    "            squared l2 norm of residual\n",
    "        \"\"\"\n",
    "        # Phys Loss\n",
    "        y_colloc_pred = self.forward(t_colloc)\n",
    "        s,e,i,r,d = y_colloc_pred.unbind(1)\n",
    "        ds_dt = caputo_l1_diff(s, self.alpha(), self.dt)\n",
    "        de_dt = caputo_l1_diff(e, self.alpha(), self.dt)\n",
    "        di_dt = caputo_l1_diff(i, self.alpha(), self.dt)\n",
    "        dr_dt = caputo_l1_diff(r, self.alpha(), self.dt)\n",
    "        dd_dt = caputo_l1_diff(d, self.alpha(), self.dt)\n",
    "\n",
    "        # calculate RHS of equation 4\n",
    "        num_living =  1 - d\n",
    "        f_s = -self.beta() * s * i / num_living\n",
    "        f_e = (self.beta() * s * i / num_living) - self.sigma() * e\n",
    "        f_i = (self.sigma() * e) - (self.gamma()+ self.mu()) * i\n",
    "        f_r = self.gamma() * i\n",
    "        f_d = self.mu() * i\n",
    "\n",
    "        # calc residuals (LHS - RHS = 0)\n",
    "        residual_s = ds_dt - f_s\n",
    "        residual_e = de_dt - f_e\n",
    "        residual_i = di_dt - f_i\n",
    "        residual_r = dr_dt - f_r\n",
    "        residual_d = dd_dt - f_d\n",
    "\n",
    "        all_residuals = torch.cat([residual_s, residual_e, residual_i, residual_r, residual_d], dim=1)\n",
    "        loss_phys = torch.mean(all_residuals**2)\n",
    "        return loss_phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae739152-8683-4e0f-96ec-316764a781ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(model, ts, ys, t_colloc, ic, optimizer, epochs=1000, pr=0):\n",
    "    \"\"\"Stage one of EpiPINN training process.\n",
    "\n",
    "    In this stage, only the weights of the neural network are trained to minimize the data and initial condition loss.\n",
    "\n",
    "    Arguments:\n",
    "        model (EpiPINN): An instantiated fractional SEIRD model to train.\n",
    "        ts (torch.tensor): Time values for time-series data.\n",
    "        ys (torch.tensor): States (s, e, i, r, d) for time-series data.\n",
    "        optimizer: Pytorch training optimizer.\n",
    "        epochs=1000 (Int): How many epochs to perform gradient descent.\n",
    "        pr=0 (Int): Print progress every pr epochs. If 0, nothing is printed.\n",
    "\n",
    "    Returns:\n",
    "        losses, losses_data, losses_ic, losses_phys\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    losses_data = []\n",
    "    losses_ic = []\n",
    "    losses_phys = []\n",
    "\n",
    "    # Ensure epidemiological parameters are not trained\n",
    "    model.raw_beta.requires_grad = False\n",
    "    model.raw_sigma.requires_grad = False\n",
    "    model.raw_gamma.requires_grad = False\n",
    "    model.raw_mu.requires_grad = False\n",
    "    model.z_alpha.requires_grad = False\n",
    "\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = model(ts)\n",
    "        \n",
    "        # Compute losses separately, then combine\n",
    "        loss_data = model.get_loss_data(ts, ys, y_data_pred=predictions)\n",
    "        loss_ic = model.get_loss_ic(t_colloc, ic, y_pred=predictions)\n",
    "        loss_phys = model.get_loss_phys(t_colloc)\n",
    "        loss = loss_data + loss_ic # Physics is not used for gradient descent\n",
    "\n",
    "        # Record losses\n",
    "        losses.append(loss.item() + loss_phys.item()) # Physics is recored, not mimimized\n",
    "        losses_data.append(loss_data.item())\n",
    "        losses_ic.append(loss_ic.item())\n",
    "        losses_phys.append(loss_phys.item())\n",
    "\n",
    "        # Adjust weights to minimize loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress if desired\n",
    "        if pr != 0 and (epoch + 1) % pr == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "        \n",
    "    return losses, losses_data, losses_ic, losses_phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d10104-a1c3-4cf5-bec2-f8e9053aa938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(model, ts, ys, t_colloc, ic, optimizer, epochs=1000, pr=0):\n",
    "    \"\"\"Stage two of EpiPINN training process.\n",
    "\n",
    "    In this stage, both the weights of the neural network and epidemiological parameters are trained to minimize the data, initial condition, and physics losses.\n",
    "\n",
    "    Arguments:\n",
    "        model (EpiPINN): An instantiated fractional SEIRD model to train.\n",
    "        ts (torch.tensor): Time values for time-series data.\n",
    "        ys (torch.tensor): States (s, e, i, r, d) for time-series data.\n",
    "        optimizer: Pytorch training optimizer.\n",
    "        epochs=1000 (Int): How many epochs to perform gradient descent.\n",
    "        pr=0 (Int): Print progress every pr epochs. If 0, nothing is printed.\n",
    "\n",
    "    Returns:\n",
    "        losses, losses_data, losses_ic, losses_phys, alphas, betas, sigmas, gammas, mus\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    losses_data = []\n",
    "    losses_ic = []\n",
    "    losses_phys = []\n",
    "    alphas = []\n",
    "    betas = []\n",
    "    sigmas = []\n",
    "    gammas = []\n",
    "    mus = []\n",
    "\n",
    "    # Ensure epidemiological parameters are trained\n",
    "    model.raw_beta.requires_grad = True\n",
    "    model.raw_sigma.requires_grad = True\n",
    "    model.raw_gamma.requires_grad = True\n",
    "    model.raw_mu.requires_grad = True\n",
    "    model.z_alpha.requires_grad = True\n",
    "\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = model(ts)\n",
    "        \n",
    "        # Compute losses separately, then combine\n",
    "        loss_data = model.get_loss_data(ts, ys, y_data_pred=predictions)\n",
    "        loss_ic = model.get_loss_ic(t_colloc, ic, y_pred=predictions)\n",
    "        loss_phys = model.get_loss_phys(t_colloc)\n",
    "        loss = loss_data + loss_ic + loss_phys\n",
    "\n",
    "        # Record losses\n",
    "        losses.append(loss.item())\n",
    "        losses_data.append(loss_data.item())\n",
    "        losses_ic.append(loss_ic.item())\n",
    "        losses_phys.append(loss_phys.item())\n",
    "\n",
    "        # Record epidemiological parameters\n",
    "        alphas.append(model.alpha().item())\n",
    "        betas.append(model.beta().item())\n",
    "        sigmas.append(model.sigma().item())\n",
    "        gammas.append(model.gamma().item())\n",
    "        mus.append(model.mu().item())\n",
    "\n",
    "        # Adjust weights to minimize loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress if desired\n",
    "        if pr != 0 and (epoch + 1) % pr == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return losses, losses_data, losses_ic, losses_phys, alphas, betas, sigmas, gammas, mus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8ce53-f989-4519-a152-99ff2b0869aa",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experiment 1: Parameter estimation with exact data\n",
    "\n",
    "This experiment is a recreation of the Mpox synthetic case found in the paper. We were unable to find the synthetic time series data $(s, e, i, r, d)(t)$ from the cited source, so we instead implemented a fractional ODE solver to generate data consistent with the provided epidemiological parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b28306a-d6a5-44f5-9e41-9bb05c2e8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "alpha_true = 0.85 # Derivative fraction used for data\n",
    "beta_true = 0.25 # Infection rate used for model\n",
    "sigma_true = 0.13 # Incubation rate used for model\n",
    "gamma_true = 0.052 # Recovery rate used for model\n",
    "mu_true = 0.005 # Death rate used for model\n",
    "y0 = np.array([0.99, 0.01, 0, 0, 0]) # Initial state: 1% exposed\n",
    "t0 = 0\n",
    "tf = 500 # 500 days\n",
    "num_step = 200 # Good ground truth from tests\n",
    "\n",
    "f = lambda t, y: np.array([\n",
    "    - beta_true * (y[0] * y[3]) / (1 - y[4]),\n",
    "    beta_true * (y[0] * y[3]) / (1 - y[4]) - sigma_true * y[1],\n",
    "    sigma_true * y[1] - (gamma_true + mu_true) * y[2],\n",
    "    gamma_true * y[2],\n",
    "    mu_true * y[2]\n",
    "])\n",
    "\n",
    "ts, ys = caputo_euler(f, alpha_true, (t0, tf), num_step, y0)\n",
    "\n",
    "ts_train = torch.tensor(ts.reshape(-1, 1), dtype=torch.float32)\n",
    "ys_train = torch.tensor(ys, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e7f4b64-0d3a-48a6-acd6-5e3d146e321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "min_alpha_guess = 0.8 # Mimimum searched derivative fraction\n",
    "alpha_guess = 0.92 # Derivative fraction used for model\n",
    "scaled_alpha_guess = (alpha_guess - min_alpha_guess) / (1.0 - min_alpha_guess) # Rescaling for search set to match range of sigmoid (0, 1)\n",
    "z_alpha_guess = np.log(scaled_alpha_guess / (1 - scaled_alpha_guess)) # Inverse sigmoid\n",
    "beta_guess = 0.2 # Infection rate used for model\n",
    "sigma_guess = 0.17 # Incubation rate used for model\n",
    "gamma_guess = 0.04 # Recovery rate used for model\n",
    "mu_guess = 0.01 # Death rate used for model\n",
    "hidden_size = 64 # Number of neurons per layer\n",
    "depth = 3 # Number of layers\n",
    "ts_colloc = torch.from_numpy(np.linspace(t0, tf, 400)).float().unsqueeze(1)\n",
    "initial_params = {\n",
    "    \"z_alpha\": z_alpha_guess,\n",
    "    \"min_alpha\": min_alpha_guess,\n",
    "    \"beta\": beta_guess,\n",
    "    \"sigma\": sigma_guess,\n",
    "    \"gamma\": gamma_guess,\n",
    "    \"mu\": mu_guess,\n",
    "    \"dt\": (tf - t0) / num_step\n",
    "}\n",
    "ic = torch.tensor(y0, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "model1 = EpiPINN(hidden_size, depth, initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa3a179-3c3f-449a-9aaf-574fbe24d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if already generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb566f-c56f-460c-a0f7-a8d46a1a91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 0.000907\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start1 = time.time()\n",
    "\n",
    "# Stage 1: weights only, without considering physics in updates\n",
    "losses1, losses1_data, losses1_ic, losses1_phys = train_stage1(\n",
    "    model1,\n",
    "    ts_train,\n",
    "    ys_train,\n",
    "    ts_colloc,\n",
    "    ic,\n",
    "    optim.Adam(model1.parameters(), lr=1e-2),\n",
    "    epochs=2000,\n",
    "    pr=100\n",
    ")\n",
    "\n",
    "# Stop timing\n",
    "stop1 = time.time()\n",
    "print(f'Complete in {stop1 - start1} seconds')\n",
    "\n",
    "# Confirm epidemiological parameters are unchanged\n",
    "print(f'alpha = {model1.alpha().item()}')\n",
    "print(f'beta = {model1.beta().item()}')\n",
    "print(f'gamma = {model1.gamma().item()}')\n",
    "print(f'sigma = {model1.sigma().item()}')\n",
    "print(f'mu = {model1.mu().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a69698-d477-48cf-9f3b-86ec916dde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start2 = time.time()\n",
    "\n",
    "# Stage 2: weights and epidemiological parameters, considering full loss\n",
    "losses2, losses2_data, losses2_ic, losses2_phys, alphas, betas, sigmas, gammas, mus = train_stage2(\n",
    "    model1,\n",
    "    ts_train,\n",
    "    ys_train,\n",
    "    ts_colloc,\n",
    "    ic,\n",
    "    optim.Adam(model1.parameters(), lr=1e-2),\n",
    "    epochs=2000,\n",
    "    pr=100\n",
    ")\n",
    "\n",
    "# Time stage 2 and cumulative\n",
    "stop2 = time.time()\n",
    "print(f'Complete in {stop2 - start2} seconds')\n",
    "\n",
    "# Confirm epidemiological parameters are unchanged\n",
    "print(f'alpha = {model1.alpha().item()}')\n",
    "print(f'beta = {model1.beta().item()}')\n",
    "print(f'gamma = {model1.gamma().item()}')\n",
    "print(f'sigma = {model1.sigma().item()}')\n",
    "print(f'mu = {model1.mu().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a8b00-0945-46c9-a3fc-7b63fcd74553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model1.state_dict(), 'experiment1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502ba52-25fa-4ff5-bb19-349ca34b421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound plot of model results, and comparison against data\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "labels = ['Full System', 's', 'e', 'i', 'r', 'd']\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "ts_val = torch.from_numpy(np.linspace(t0, tf, 1000)).float().unsqueeze(1)\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    ys_val = model1(ts_val)\n",
    "\n",
    "# Full system\n",
    "for i in range(5):\n",
    "    axs[0, 0].plot(ts_val, ys_val[:, i], label=labels[i + 1])\n",
    "\n",
    "# Individial plots\n",
    "for i in range(5):\n",
    "    axs[(i + 1) // 3, (i + 1) % 3].plot(ts_val, ys_val[:, i], label='EpiPINN', color=colors[i])\n",
    "    axs[(i + 1) // 3, (i + 1) % 3].plot(ts, ys[:, i], label='data/FDE', color='k')\n",
    "\n",
    "for i in range(6):\n",
    "    axs[i // 3, i % 3].set_title(labels[i])\n",
    "    axs[i // 3, i % 3].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453c751-88a1-42b4-a9b9-b95e54d88574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "l1 = len(losses1)\n",
    "l2 = len(losses2)\n",
    "epochs1 = range(l1)\n",
    "epochs2 = range(l1, l1 + l2)\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(epochs1, losses1, label='total loss', color=colors[0])\n",
    "ax.semilogy(epochs1, losses1_data, label='data loss', color=colors[1])\n",
    "ax.semilogy(epochs1, losses1_ic, label='initial condition loss', color=colors[2])\n",
    "ax.semilogy(epochs1, losses1_phys, label='physics loss', color=colors[3])\n",
    "ax.semilogy(epochs2, losses2, color=colors[0], ls='--')\n",
    "ax.semilogy(epochs2, losses2_data, color=colors[1], ls='--')\n",
    "ax.semilogy(epochs2, losses2_ic, color=colors[2], ls='--')\n",
    "ax.semilogy(epochs2, losses2_phys, color=colors[3], ls='--')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c82d63-0d87-4a95-a7a3-e5d8c13f82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Parameter convergence\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "axs[0, 0].plot(epochs2, alphas)\n",
    "axs[0, 0].plot(epochs2, alpha_true * np.ones(l2), color='k', ls='--')\n",
    "axs[0, 0].set_title('alpha')\n",
    "\n",
    "axs[0, 1].plot(epochs2, betas)\n",
    "axs[0, 1].plot(epochs2, beta_true * np.ones(l2), color='k', ls='--')\n",
    "axs[0, 1].set_title('beta')\n",
    "\n",
    "axs[0, 2].plot(epochs2, sigmas)\n",
    "axs[0, 2].plot(epochs2, sigma_true * np.ones(l2), color='k', ls='--')\n",
    "axs[0, 2].set_title('sigma')\n",
    "\n",
    "axs[1, 0].plot(epochs2, gammas)\n",
    "axs[1, 0].plot(epochs2, gamma_true * np.ones(l2), color='k', ls='--')\n",
    "axs[1, 0].set_title('gamma')\n",
    "\n",
    "axs[1, 1].plot(epochs2, mus)\n",
    "axs[1, 1].plot(epochs2, mu_true * np.ones(l2), color='k', ls='--')\n",
    "axs[1, 1].set_title('mu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509ea4d-f3da-4176-9f15-304ea067be9e",
   "metadata": {},
   "source": [
    "### Experiment 2: Parameter estimation with noisy data\n",
    "\n",
    "This experiment is also a recreation of the Mpox synthetic case found in the paper. Now random noise is added to the data to test the model's abitilty to recover the ideal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283fa49-1ced-4056-bfcd-e8ba72d924ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciml-env",
   "language": "python",
   "name": "sciml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
